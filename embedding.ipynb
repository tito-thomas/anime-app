{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import import_ipynb #requires pip install\n",
    "from joblib import dump, load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(r\"D:\\dataset\\encoding\\user_frame.csv\")\n",
    "anime = pd.read_csv(r\"D:\\dataset\\encoding\\anime_frame.csv\")\n",
    "ratings = pd.read_csv(r\"D:\\dataset\\encoding\\ratings_frame.csv\")\n",
    "#Function to turn \"genres\" columns from string into list\n",
    "def string_to_list(genres):\n",
    "    return eval(genres)\n",
    "\n",
    "users[\"fav_genres\"] = users[\"fav_genres\"].apply(string_to_list)\n",
    "anime[\"genre\"] = anime[\"genre\"].apply(string_to_list)\n",
    "ratings[\"genre\"] = ratings[\"genre\"].apply(string_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build genre list\n",
    "def split(genres):\n",
    "    return str(genres).split(\",\")\n",
    "\n",
    "def build_genre_list(dataframe):\n",
    "    #Updating the set to get the individual genre names in the set instead of the original list of genres\n",
    "    genre_set = set()\n",
    "    for i in dataframe: \n",
    "        print(i)\n",
    "        genre_set.update(i)\n",
    "    result = list(genre_set)\n",
    " \n",
    "    #Removing blank spaces\n",
    "    final_result = []\n",
    "    for i in result:\n",
    "        if i!=\"nan\":\n",
    "            #new = i.replace(\" \", \"\")\n",
    "            new = i.strip()\n",
    "            final_result.append(new)\n",
    "\n",
    "    #Removing duplicates now that spaces have been removed\n",
    "    genre_list = list(set(final_result))\n",
    "    return genre_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize users records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "user_cols = users.loc[:, [\"username\", \"experience\",\"gender\", \"generation\",  \"fav_anime_period\"]]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "#Function to create a separate feature for each column and vectorize\n",
    "def genre_columns():\n",
    "\n",
    "    user_genres = users.loc[:, [\"username\", \"fav_genres\"]]\n",
    "    genre_list = build_genre_list(user_genres[\"fav_genres\"])\n",
    "\n",
    "    for i in genre_list:\n",
    "        user_cols[i] = 0\n",
    "\n",
    "    for x, y in user_genres.iterrows():\n",
    "        user_row = user_cols.loc[user_cols[\"username\"] == y[\"username\"]]\n",
    "        #print(y)\n",
    "        #encode each of the three favourite genres with value: 1\n",
    "        user_row[y[\"fav_genres\"][0]] = 1\n",
    "        user_row[y[\"fav_genres\"][1]] = 1\n",
    "        user_row[y[\"fav_genres\"][2]] = 1\n",
    "\n",
    "        user_cols.loc[user_cols[\"username\"] == y[\"username\"]] = user_row\n",
    "\n",
    "    genre_cols = user_cols.drop(columns=[\"experience\",\"gender\", \"generation\",\"fav_anime_period\"])\n",
    "    return genre_cols\n",
    "\n",
    "#Vectorise features of a single user\n",
    "def vectorize_single(user):\n",
    "    #Get row of one user\n",
    "    user_row = user_cols.loc[user_cols[\"username\"] == user]\n",
    "    user_vector = user_row.copy()\n",
    "    \n",
    "    #Fit the encoder with the data of original df: user_cols\n",
    "    for col in user_cols.columns:\n",
    "        enc = encoder.fit(user_cols[col])\n",
    "        #save the conder to file\n",
    "\n",
    "        dump(enc, f\"{col}.pkl\")\n",
    "        #print(list(enc.classes_))\n",
    "        user_vector[col]=enc.transform(user_vector[col]) #encode each column of target user\n",
    "\n",
    "    return user_vector\n",
    "\n",
    "#Vectorise features of all users in the dataframe\n",
    "def vectorize_all():\n",
    "    all_users = user_cols.copy()\n",
    "\n",
    "    for user in user_cols[\"username\"]:\n",
    "        user_vector = vectorize_single(user)\n",
    "        all_users.loc[all_users[\"username\"] == user] = user_vector\n",
    "    #Make sure username field is in string form for identifier    \n",
    "    all_users[\"username\"] = user_cols[\"username\"]\n",
    "    #K-Nearest Neighbours\n",
    "    #nb = KNeighborsClassifier(n_neighbors=5)\n",
    "    return all_users\n",
    "\n",
    "#Construct final vector of encoded features\n",
    "def construct_vectors():\n",
    "    string_features = vectorize_all()\n",
    "    genre_features = genre_columns()\n",
    "    final_vector = pd.merge(string_features, genre_features, on = \"username\")\n",
    "    return final_vector\n",
    "\n",
    "#result = construct_vectors()\n",
    "#result.to_csv(r\"D:\\dataset\\encoding\\user_vectors.csv\", index=False)\n",
    "#genre_columns()\n",
    "#vectorize_all()\n",
    "#user_cols\n",
    "# need to translate genres that user is asked to anime genres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'New-Genn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\titot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:224\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\titot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[1;34m(values, uniques)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[39m=\u001b[39m _nandict({val: i \u001b[39mfor\u001b[39;00m i, val \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([table[v] \u001b[39mfor\u001b[39;49;00m v \u001b[39min\u001b[39;49;00m values])\n",
      "File \u001b[1;32mc:\\Users\\titot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:164\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m table \u001b[39m=\u001b[39m _nandict({val: i \u001b[39mfor\u001b[39;00m i, val \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(uniques)})\n\u001b[1;32m--> 164\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([table[v] \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m values])\n",
      "File \u001b[1;32mc:\\Users\\titot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:158\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnan_value\n\u001b[1;32m--> 158\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'New-Genn'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test \u001b[39m=\u001b[39m load(\u001b[39m\"\u001b[39m\u001b[39mfav_anime_period.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m x \u001b[39m=\u001b[39m test\u001b[39m.\u001b[39;49mtransform([\u001b[39m\"\u001b[39;49m\u001b[39mNew-Genn\u001b[39;49m\u001b[39m\"\u001b[39;49m])[\u001b[39m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m x\n",
      "File \u001b[1;32mc:\\Users\\titot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:142\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 142\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    144\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    145\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    146\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    147\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    148\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\titot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:139\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m _num_samples(y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    137\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray([])\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m _encode(y, uniques\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_)\n",
      "File \u001b[1;32mc:\\Users\\titot\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_encode.py:226\u001b[0m, in \u001b[0;36m_encode\u001b[1;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[39mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[0;32m    225\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 226\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my contains previously unseen labels: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(e)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    228\u001b[0m     \u001b[39mif\u001b[39;00m check_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: y contains previously unseen labels: 'New-Genn'"
     ]
    }
   ],
   "source": [
    "test = load(\"fav_anime_period.pkl\")\n",
    "x = test.transform([\"New-Gen\"])[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>experience</th>\n",
       "      <th>gender</th>\n",
       "      <th>generation</th>\n",
       "      <th>fav_anime_period</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9301</th>\n",
       "      <td>6443</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      username  experience  gender  generation  fav_anime_period\n",
       "9301      6443           3       1           1                 1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_single(\"badking95\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anime_vector = anime.loc[:, [\"airing\",\"studio\", \"genre\", \"time_period\", \"fame\"]]\n",
    "anime_vector\n",
    "#anime\n",
    "#anime_genres = users.loc[:, \"genre\"]\n",
    "#genre_list = build_genre_list(anime_genres)\n",
    "#for i in genre_list:\n",
    "            #user_cols[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = pd.read_csv(r\"D:\\dataset\\encoding\\collab_frame.csv\")\n",
    "\n",
    "full_ratings = rf[rf[\"username\"].isin(users[\"username\"])] #ensure that only ratings from users that are in the users dataframe are included\n",
    "\n",
    "print(len(full_ratings))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build user-item matrix with ratings (collaborative filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#not using genres because this should focus on ratings \n",
    "anime_matrix = pd.DataFrame(columns = users[\"username\"].values)\n",
    "anime_matrix[\"anime_id\"] = anime[\"anime_id\"]\n",
    "anime_matrix.set_index(\"anime_id\")\n",
    "col = anime_matrix.pop(\"anime_id\")\n",
    "anime_matrix.insert(0, col.name, col)\n",
    "#anime_matrix.iloc[:]=0\n",
    "rating_matrix = pd.DataFrame(0, columns=anime_matrix.columns, index=anime_matrix.index) #set all values to 0 by default\n",
    "rating_matrix[\"anime_id\"] = anime[\"anime_id\"]\n",
    "\n",
    "#Filter rows in rating_matrix to only include anime that are also in the full_ratings table\n",
    "rating_matrix = rating_matrix[rating_matrix[\"anime_id\"].isin(full_ratings[\"anime_id\"])]\n",
    "\n",
    "for show in rating_matrix.iterrows():\n",
    "    show_id = int(show[1][\"anime_id\"])\n",
    "    #show_id = int(full_anime.loc[full_anime[\"title\"]==show[0]][\"anime_id\"])\n",
    "    anime_ratings = full_ratings.groupby(\"anime_id\").get_group(show_id)\n",
    "    usernames = list(anime_ratings[\"username\"])\n",
    "    scores = list(anime_ratings[\"my_score\"]) #normalise ratings\n",
    "    #print(anime_ratings)\n",
    "    mean = list(anime_ratings[\"mean_score\"])\n",
    "    #mean = float(users.loc[users[\"username\"]==users[\"stats_mean_score\"]])\n",
    "    norm = np.subtract(scores, mean)\n",
    "    #print(norm)\n",
    "    #print(usernames)\n",
    "    #print(scores)      \n",
    "\n",
    "    rating_matrix.loc[rating_matrix[\"anime_id\"]==show_id,usernames] = scores #test with raw scores and normalised scores\n",
    "#adding extra features to be used in combination with ratings for each show\n",
    "extra_features = anime.loc[:, [\"anime_id\", \"type\", \"studio\", \"time_period\", \"fame\"]]\n",
    "rating_matrix = rating_matrix.merge(extra_features, on=\"anime_id\")\n",
    "\n",
    "#Label encode the extra features\n",
    "collab_encoder = LabelEncoder()\n",
    "for col in extra_features.columns[1:]:\n",
    "    col_fit = collab_encoder.fit(rating_matrix[col])\n",
    "    rating_matrix[col] = col_fit.transform(rating_matrix[col])\n",
    "\n",
    "rating_matrix.to_csv(r\"D:\\dataset\\encoding\\collab_scores.csv\", index = False)\n",
    "rating_matrix\n",
    "#extra_features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2fbeb981500814da07500025845562e048eba632abcc03a88daa93daf0265bae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
